{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinkful's Iterate and Evaluate Your Classifier Challenge\n",
    "\n",
    "Data Source: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "Using evaluation techniques to look at my classifier's performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "df = pd.read_csv('sentiment_labelled_sentences/amazon_cells_labelled.txt',\n",
    "                 delimiter='\\t',\n",
    "                 header=None,\n",
    "                 names=['text', 'score'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[487,  13],\n",
       "       [405,  95]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first attempt - good words only\n",
    "#cast everything to lowercase to make it easier to match words\n",
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "    \n",
    "#picking keywords associated with good reviews\n",
    "keywords = ['good', 'great', 'excellent', 'beautiful', 'best', 'satisfied']\n",
    "\n",
    "for key in keywords:\n",
    "    # Noted adding spaces around keyword to make sure matching on word and not pattern\n",
    "    df[str(key)] = df.text.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=True\n",
    "    )\n",
    "\n",
    "data = df[keywords]\n",
    "target = df['score']\n",
    "    \n",
    "# Our data is binary / boolean so using Bernoulli classifier.\n",
    "\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "    \n",
    "total_points = data.shape[0]\n",
    "wrong_points = (target != y_pred).sum()\n",
    "score = bnb.score(data, target)\n",
    "    \n",
    "    \n",
    "confusion = confusion_matrix(target, y_pred)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.585\n",
      "Testing on Sample: 0.582\n"
     ]
    }
   ],
   "source": [
    "# Test your model with different holdout groups.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the scores here are very similar it doesn't seem like the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58, 0.59, 0.59, 0.57, 0.59, 0.6 , 0.57, 0.58, 0.61, 0.53])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross validation - leave one out\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8, 492],\n",
       "       [  0, 500]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second attempt - bad words only\n",
    "#cast everything to lowercase to make it easier to match words\n",
    "\n",
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "    \n",
    "#picking keywords associated with bad reviews\n",
    "keywords = ['unsatisfactory', 'disappointed', 'disappoint', 'junk', 'painful', 'unusable', 'negative']\n",
    "\n",
    "for key in keywords:\n",
    "    # Noted adding spaces around keyword to make sure matching on word and not pattern\n",
    "    df[str(key)] = df.text.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "\n",
    "data = df[keywords]\n",
    "target = df['score']\n",
    "    \n",
    "# Our data is binary / boolean so using Bernoulli classifier.\n",
    "\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "    \n",
    "total_points = data.shape[0]\n",
    "wrong_points = (target != y_pred).sum()\n",
    "score = bnb.score(data, target)\n",
    "    \n",
    "confusion = confusion_matrix(target, y_pred)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.47\n",
      "Testing on Sample: 0.508\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52, 0.5 , 0.51, 0.5 , 0.51, 0.5 , 0.5 , 0.5 , 0.5 , 0.52])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross validation - leave one out\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[487,  13],\n",
       "       [405,  95]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#third attempt - good and bad words together\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('sentiment_labelled_sentences/amazon_cells_labelled.txt',\n",
    "                 delimiter='\\t',\n",
    "                 header=None,\n",
    "                 names=['text', 'score'])\n",
    "\n",
    "#cast everything to lowercase to make it easier to match words\n",
    "\n",
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "    \n",
    "#picking keywords associated with good reviews\n",
    "good = ['good', 'great', 'excellent', 'beautiful', 'best', 'satisfied']\n",
    "    \n",
    "#picking keywords associated with bad reviews\n",
    "bad = ['unsatisfactory', 'disappointed', 'disappoint', 'junk', 'painful', 'unusable', 'negative']\n",
    "    \n",
    "keywords = good + bad\n",
    "\n",
    "for key in bad:\n",
    "    # Noted adding spaces around keyword to make sure matching on word and not pattern\n",
    "    df[str(key)] = df.text.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "    \n",
    "for key in good:\n",
    "    # Noted adding spaces around keyword to make sure matching on word and not pattern\n",
    "    df[str(key)] = df.text.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=True\n",
    "    )\n",
    "    \n",
    "\n",
    "data = df[keywords]\n",
    "target = df['score']\n",
    "    \n",
    "# Our data is binary / boolean so using Bernoulli classifier.\n",
    "\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "    \n",
    "total_points = data.shape[0]\n",
    "wrong_points = (target != y_pred).sum()\n",
    "score = bnb.score(data, target)\n",
    "    \n",
    "confusion = confusion_matrix(target, y_pred)\n",
    "confusion\n",
    "\n",
    "#noting here this is no better than the good words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.585\n",
      "Testing on Sample: 0.582\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split to create the necessary training and test groups\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58, 0.59, 0.59, 0.57, 0.59, 0.6 , 0.57, 0.58, 0.61, 0.53])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross validation - leave one out\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting variables just in case\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[487,  13],\n",
       "       [411,  89]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fourth attempt - I can't imagine this making it better but not casting to lowercase\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('sentiment_labelled_sentences/amazon_cells_labelled.txt',\n",
    "                 delimiter='\\t',\n",
    "                 header=None,\n",
    "                 names=['text', 'score'])\n",
    "\n",
    "#picking keywords associated with good reviews\n",
    "good = ['good', 'great', 'excellent', 'beautiful', 'best', 'satisfied']\n",
    "    \n",
    "#picking keywords associated with bad reviews\n",
    "bad = ['unsatisfactory', 'disappointed', 'disappoint', 'junk', 'painful', 'unusable', 'negative']\n",
    "    \n",
    "keywords = good + bad\n",
    "\n",
    "for key in bad:\n",
    "    # Noted adding spaces around keyword to make sure matching on word and not pattern\n",
    "    df[str(key)] = df.text.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "    \n",
    "for key in good:\n",
    "    # Noted adding spaces around keyword to make sure matching on word and not pattern\n",
    "    df[str(key)] = df.text.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=True\n",
    "    )\n",
    "    \n",
    "\n",
    "data = df[keywords]\n",
    "target = df['score']\n",
    "    \n",
    "# Our data is binary / boolean so using Bernoulli classifier.\n",
    "\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "    \n",
    "total_points = data.shape[0]\n",
    "wrong_points = (target != y_pred).sum()\n",
    "score = bnb.score(data, target)\n",
    "    \n",
    "confusion = confusion_matrix(target, y_pred)\n",
    "confusion\n",
    "\n",
    "#noting exactly the same as take three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.565\n",
      "Testing on Sample: 0.576\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split to create the necessary training and test groups\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57, 0.58, 0.59, 0.56, 0.59, 0.59, 0.57, 0.57, 0.61, 0.52])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross validation - leave one out\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[484,  16],\n",
       "       [379, 121]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fifth attempt - good words only + more good words\n",
    "\n",
    "#cast everything to lowercase to make it easier to match words\n",
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "    \n",
    "#picking keywords associated with good reviews\n",
    "keywords = ['good', 'great', 'excellent', 'beautiful', 'best', 'satisfied',\n",
    "           'love', 'loved', 'impressed', 'good quality', 'nice', '']\n",
    "\n",
    "for key in keywords:\n",
    "    # Noted adding spaces around keyword to make sure matching on word and not pattern\n",
    "    df[str(key)] = df.text.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=True\n",
    "    )\n",
    "\n",
    "data = df[keywords]\n",
    "target = df['score']\n",
    "    \n",
    "# Our data is binary / boolean so using Bernoulli classifier.\n",
    "\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "    \n",
    "total_points = data.shape[0]\n",
    "wrong_points = (target != y_pred).sum()\n",
    "score = bnb.score(data, target)\n",
    "    \n",
    "    \n",
    "confusion = confusion_matrix(target, y_pred)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.6\n",
      "Testing on Sample: 0.605\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split to create the necessary training and test groups\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6 , 0.63, 0.63, 0.58, 0.59, 0.62, 0.6 , 0.59, 0.62, 0.54])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross validation - leave one out\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth classifier seems to work the best, as it has the most words to match on.  Having more words to match seems to impact performance.  None of my models seem to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
